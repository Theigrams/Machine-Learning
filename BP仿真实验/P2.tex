\newpage
\section{奇偶判别问题}
对于一串长度为$N$的二进制输入，判断串中1的个数是否为奇数，解决此类问题的神经网络通常构架为：$[N,N,1]$，即$N$个输入单元，$N$个隐藏单元，1个输出单元。

下面以$N=4$为例来解决这个问题。

首先是生成数据：将从0到$2^N-1$的数字转化为二进制，储存为输入向量T，然后生成对应的期望输出P.

\begin{lstlisting}
N=4;
T = abs(dec2bin(0:(2^N-1), N))-48;
P=mod(sum(T, 2),2);
save data2.mat T P
\end{lstlisting}

然后将之前编好之前的三层神经网络程序模板稍微改动一下，可得到运行结果如下：

\begin{figure}[H]
\centering
\includegraphics[width=10cm]{fig/parity.pdf}
\caption{cost函数下降过程}
\end{figure}

\newpage
\begin{lstlisting}
w2 =
    5.6930    5.6926    5.6940   -5.4309
    4.0606    4.0606    4.0606   -4.8899
    6.4270    6.4271    6.4268   -6.9895
    7.7000    7.7003    7.6988   -7.9590

w3 =
  -12.8905   14.3991  -14.9507   14.5050

b2 =
    2.7115
  -10.2916
   -9.1962
   -3.5978

b3 =
    5.7940

a3 =
  1 至 4 列
    0.0027    0.9933    0.9982    0.0021
  5 至 8 列
    0.9982    0.0021    0.0034    0.9986
  9 至 12 列
    0.9982    0.0021    0.0034    0.9986
  13 至 16 列
    0.0034    0.9986    0.9931    0.0020

ans =
    0.0028
\end{lstlisting}

仔细观察神经网络的权值，可以发现一个特点：$w_2$每一行的左边三列都是近似相等的，而最右边一列则近似为它们的相反数，并且每一行的绝对值相差都不大，在$w_3$中，则表现为正负号交替，且绝对值也相差不大，这与作者给出的样例类似\footnote{样例中为正负1交替}，这验证了作者的观点：这这样的网络模型中，由学习规则所创建的内部表示将会使得被激活的隐藏神经元的数量等于输入中的“1”的数量，由$w_3$中正负号交替的性质，当有奇数个隐藏神经元被激活时，输出为1，偶数个隐藏神经元被激活时，输出为0.输出层的神经元能否被激活只依赖于激活隐藏神经元的数量，而不在于哪个输入神经元是否被激活，这正是奇偶性所要求的编码。

\newpage
\section{编码问题}
\textbf{编码问题}:将一组正交input pattern映射到一组正交output pattern.
\subsection{编码问题1}
\begin{table}[!tbh]
\caption{\quad 编码问题1}
\centering
\begin{tabular}{c c c}  \hline
\qquad 输入\,\,\,\,\,\,\,\,\,\,\,\, &	&\qquad 输出\,\,\,\,\,\,\,\,\,\,\,\, \\ \hline
10000000&$\rightarrow$&10000000\\
01000000&$\rightarrow$&01000000\\
00100000&$\rightarrow$&00010000\\
00010000&$\rightarrow$&00010000\\
00001000&$\rightarrow$&00001000\\
00000100&$\rightarrow$&00000100\\
00000010&$\rightarrow$&00000010\\
00000001&$\rightarrow$&00000001\\\hline
\end{tabular}
\label{Tcode1}
\end{table}

这一类编码问题的构架为$[N,\log_2N,N]$，在这种情况下，我们要通过隐藏神经元(hidden units)给每个N位的input pattern编码，将其映射到$\log_2N$位的二进制模式，然后再将其解码至N位的output pattern.
\begin{lstlisting}
a3 =
0.9797    0.0000    0.0000    0.0003    0.0000    0.0083    0.0069    0.0115
0.0000    0.9659    0.0370    0.0000    0.0005    0.0000    0.0058    0.0009
0.0000    0.0224    0.9421    0.0322    0.0024    0.0000    0.0000    0.0000
0.0000    0.0000    0.0209    0.9446    0.0000    0.0209    0.0000    0.0071
0.0000    0.0042    0.0105    0.0001    0.9777    0.0151    0.0005    0.0000
0.0095    0.0000    0.0000    0.0169    0.0111    0.9666    0.0000    0.0000
0.0206    0.0345    0.0000    0.0000    0.0001    0.0000    0.9567    0.0046
0.0040    0.0022    0.0016    0.0078    0.0000    0.0000    0.0000    0.9876

ans =
    0.0761
\end{lstlisting}

\begin{figure}[H]
\centering
\includegraphics[width=10cm]{fig/encode1.pdf}
\caption{cost函数下降过程}
\end{figure}

\subsection{编码问题2}
\begin{table}[!tbh]
\caption{\quad 编码问题2}
\centering
\begin{tabular}{c c c}  \hline
\qquad 输入\,\,\,\,\,\,\,\,\,\,\,\, &	&\qquad 输出\,\,\,\,\,\,\,\,\,\,\,\, \\ \hline
00&$\rightarrow$&1000\\
01&$\rightarrow$&0100\\
10&$\rightarrow$&0010\\
11&$\rightarrow$&0001\\\hline
\end{tabular}
\label{Tcode24}
\end{table}

该类编码问题中，我们要把两单元的分散式表示转化成四单元的局部表示，离散输入模式的相似性结构将不会被保留到局部输出表示中。为了解决这个问题，系统首先要把输入模式的分散式表示转化成不同的激活值，对应于不同输入模式下单一隐藏神经元的中间值，然后将其转化到另一个分散式表示，最后转换为局部表示。（注意：此处的分散式表示Distributed Representation指一个个体用几个编码单元而不是一个编码单元表示，即一个个体分布在几个编码单元上）。

因此此题中要求的构架为$[2,1,4,4]$，不能继续套用之前的三层神经网络模板了，于是我用结构体编写了一个N层神经网络的模板，因为MATLAB中的三维矩阵不能储存维数不一致的矩阵，所以只好用结构数组，代码更直观的同时，速度却慢了很多。

\begin{lstlisting}
L(num_layers).a =
    0.9859    0.0067    0.0000    0.0068
    0.0068    0.9940    0.0000    0.0000
    0.0000    0.0000    0.9950    0.0045
    0.0105    0.0000    0.0108    0.9824

ans =
    0.0224
\end{lstlisting}

\begin{figure}[H]
\centering
\includegraphics[width=10cm]{fig/encode2.pdf}
\caption{cost函数下降过程}
\end{figure}

\newpage
\section{对称问题}
\textbf{对称问题:}对于一串二进制输入，判断是否是中心对称的.

\begin{figure}[H]
\centering
\includegraphics[width=10cm]{fig/symmetry0.pdf}
\caption{一个解决对称问题的网络结构}
\end{figure}
从例子中可以看出：由于权值关于上下左右的对称性，当输入并非对称时，上下两个hidden units至少有一个会被激活，从而导致输出神经元被关闭。

此外，书中还指出：每条边的权值之比为1:2:4，这使得右边三个神经元发送到隐藏层的总激活值唯一，使得左侧的非对称输入难以精确平衡抵消掉。最后，只有当两个hidden units都处于未激活状态时，输出神经元才会被激活。

将学习速率调为1后，得到了很好的输出结果如下：

观察输出的$w_2,w_3$，每条边的权值之比确实精确为1:2:4，权值、偏置也具有对称性，很好地吻合了Hinton的结论.
\begin{lstlisting}
L(2).w =
    3.6554    7.2908   14.5102  -14.5128   -7.2938   -3.6593
   -3.4807   -6.9357  -13.7993   13.7964    6.9322    3.4746

L(3).w =
  -19.3936  -19.6933

L(2).b =
   -2.4196
   -2.3128

L(3).b =
    8.7942
   
ans =
   2.0560e-04
\end{lstlisting}

\begin{figure}[H]
\centering
\includegraphics[width=7cm]{fig/symmetry1.pdf}
\caption{cost函数下降过程}
\end{figure}

此外，在调试程序的过程中，还添加了minibatch法，取minibatch的size=40，也得到了不错的输出:
\begin{figure}[H]
\centering
\includegraphics[width=7cm]{fig/symmetry2.pdf}
\caption{minibatch法}
\end{figure}